{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8YPK4UckA-LB"},"outputs":[],"source":["import random\n","\n","# 读取train数据，划分验证集\n","train_index = []\n","train_label = []\n","valid_index = []\n","valid_label = []\n","with open('Lab5_Data/train.txt', 'r') as f:\n","    line  =  f.readline() \n","    while line: \n","      strs = line.split(',')\n","      if strs[0] != 'guid':\n","        if random.randint(0,9) < 8: \n","            train_index.append(strs[0])\n","            train_label.append(strs[1].strip())\n","        else:\n","            valid_index.append(strs[0])\n","            valid_label.append(strs[1].strip())\n","      line = f.readline()\n","\n","# 读取测试数据\n","test_index = []\n","with open('Lab5_Data/test_without_label.txt', 'r') as f:\n","    line  =  f.readline() \n","    while line: \n","      strs = line.split(',')\n","      if strs[0] != 'guid':\n","        test_index.append(strs[0])\n","      line = f.readline()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fdzmgrg-A-LE","outputId":"30b75a73-94dd-4083-ad48-6820760bfe26"},"outputs":[{"name":"stdout","output_type":"stream","text":["['4597', '26', '4383', '212', '2626', '3042', '4713', '2073', '2688', '3143']\n","['negative', 'neutral', 'negative', 'positive', 'positive', 'neutral', 'positive', 'negative', 'negative', 'positive']\n","['2020', '4956', '3624', '2855', '4784', '3244', '725', '138', '1562', '3888']\n","['positive', 'positive', 'positive', 'positive', 'neutral', 'positive', 'positive', 'positive', 'positive', 'positive']\n","['8', '1576', '2320', '4912', '3821', '1306', '4555', '259', '3216', '881']\n","511\n"]}],"source":["print(train_index[:10])\n","print(train_label[:10])\n","print(valid_index[:10])\n","print(valid_label[:10])\n","print(test_index[:10])\n","print(len(test_index))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"suPMydo3A-LF","outputId":"b6d7de33-ea1a-4aa3-f2b3-95517441f37f"},"outputs":[{"name":"stdout","output_type":"stream","text":["3206\n","[0, 1, 0, 2, 2, 1, 2, 0, 0, 2]\n","794\n","[2, 2, 2, 2, 1, 2, 2, 2, 2, 2]\n"]}],"source":["# 将标签转为数字\n","label_to_index = {'negative':0, 'neutral':1, 'positive':2}\n","index_to_label = {0:'negative', 1:'neutral', 2:'positive'}\n","\n","train_label_index = [label_to_index[item] for item in train_label]\n","valid_label_index = [label_to_index[item] for item in valid_label]\n","print(len(train_label_index))\n","print(train_label_index[:10])\n","print(len(valid_label_index))\n","print(valid_label_index[:10])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L41XknW6A-LF","outputId":"7584c7cb-53d0-4bc9-f506-5faee69a4cfb"},"outputs":[{"name":"stdout","output_type":"stream","text":["3206\n","['RT @AmitSwami77: The conspirators have an evil eye & are now set to physically attack Asaram Bapu Ji! #WeDemandSafety4Bapuji http://t.co/N8\\n']\n","794\n","['# # #abandoned #дäƤˤȿꤿ #дäˤȿꤿ \\n']\n","511\n","['Energetic training today with our San Antonio New Dollars/New Partners trainees \\n']\n"]}],"source":["# 根据上面划分的id读取具体数据\n","train_text = []\n","for index in train_index:\n","    file = index + '.txt'\n","    with open('Lab5_Data/data/' + file, 'r', encoding='utf-8', errors='ignore') as f:\n","        line = f.readline() \n","        train_text.append(line)\n","print(len(train_text))\n","print(train_text[:1])\n","\n","valid_text = []\n","for index in valid_index:\n","    file = index + '.txt'\n","    with open('Lab5_Data/data/' + file, 'r', encoding='utf-8', errors='ignore') as f:\n","        line = f.readline() \n","        valid_text.append(line)\n","print(len(valid_text))\n","print(valid_text[:1])\n","\n","test_text = []\n","for index in test_index:\n","    file = index + '.txt'\n","    with open('Lab5_Data/data/' + file, 'r', encoding='utf-8', errors='ignore') as f:\n","        line = f.readline() \n","        test_text.append(line)\n","print(len(test_text))\n","print(test_text[:1])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SVl5QpDUA-LG","outputId":"4e36c620-86f9-4e47-c456-f46265203d80"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'\\\\', ';', '.', '_', '\"', '-', '>', '/', '!', '^', '&', '<', '@', '[', '=', '|', '~', ')', '%', ',', '*', '#', ':', '?', \"'\", '`', ']', '(', '{', '}', '+', '$'}\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to C:\\Users\\Guo\n","[nltk_data]     Teng\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to C:\\Users\\Guo\n","[nltk_data]     Teng\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"name":"stdout","output_type":"stream","text":["{'id': '4597', 'text': ['RT', 'AmitSwami77', 'The', 'conspirators', 'evil', 'eye', 'set', 'physically', 'attack', 'Asaram', 'Bapu', 'Ji', 'WeDemandSafety4Bapuji', 'http', '//t.co/N8'], 'label': 0}\n","{'id': '2020', 'text': ['abandoned', 'дäƤˤȿꤿ', 'дäˤȿꤿ'], 'label': 2}\n","{'id': '8', 'text': ['Energetic', 'training', 'today', 'San', 'Antonio', 'New', 'Dollars/New', 'Partners', 'trainees']}\n"]}],"source":["# 需要先将文本tokenize\n","import nltk\n","import string \n","nltk.download('punkt')\n","nltk.download('stopwords')\n","from nltk.tokenize import word_tokenize\n","from nltk.corpus import stopwords\n","\n","punctuations = set(string.punctuation)\n","stopWords = set(stopwords.words('english'))\n","\n","print(punctuations)\n","\n","def tokenizer(str):\n","    words = word_tokenize(str)\n","    wordsFiltered = [word for word in words if word not in punctuations and word not in stopWords]\n","    return wordsFiltered\n","\n","# 转为dict，便于写入jsonlines文件\n","def convert_dict_list(tokenizer, id, text, label=[], need_label=False):\n","    dict_list = []\n","    if need_label:\n","        assert len(text) == len(label)\n","        for i in range(0, len(text)):\n","            dict_list.append({'id': id[i], 'text': tokenizer(text[i]), 'label': label[i]})\n","    else:\n","        for i in range(0, len(text)):\n","            dict_list.append({'id': id[i], 'text': tokenizer(text[i])})\n","    return dict_list\n","\n","train_dict_list = convert_dict_list(id=train_index, text=train_text, label=train_label_index, tokenizer=tokenizer, need_label=True)\n","valid_dict_list = convert_dict_list(id=valid_index, text=valid_text, label=valid_label_index, tokenizer=tokenizer, need_label=True)\n","test_dict_list = convert_dict_list(id=test_index, text=test_text, tokenizer=tokenizer)\n","print(train_dict_list[0])\n","print(valid_dict_list[0])\n","print(test_dict_list[0])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ekdCiabpA-LG","outputId":"61046c93-0fec-4976-a560-4b09ed98d05b"},"outputs":[{"name":"stdout","output_type":"stream","text":["3206\n","794\n","511\n","3202\n","794\n","511\n"]}],"source":["# 观察文本中tokenize后最长的长度\n","print(len(train_dict_list))\n","print(len(valid_dict_list))\n","print(len(test_dict_list))\n","train_dict_list = [item for item in train_dict_list if len(item['text']) > 0]\n","valid_dict_list = [item for item in valid_dict_list if len(item['text']) > 0]\n","test_dict_list = [item for item in test_dict_list if len(item['text']) > 0]\n","print(len(train_dict_list))\n","print(len(valid_dict_list))\n","print(len(test_dict_list))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w_ai-AXTA-LH"},"outputs":[],"source":["import jsonlines\n","\n","# 写入jsonlines文件\n","def save_to_jsonl(dict_list, filename):\n","    with jsonlines.open('jsonl/'+filename, mode='w') as writer:\n","        for line in dict_list:\n","            writer.write(line)\n","\n","save_to_jsonl(train_dict_list, 'train.jsonl')\n","save_to_jsonl(valid_dict_list, 'valid.jsonl')\n","save_to_jsonl(test_dict_list, 'test.jsonl')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tOLmNuCjA-LI"},"outputs":[],"source":["train_dict_len = [len(item['text']) for item in train_dict_list]\n","valid_dict_len = [len(item['text']) for item in train_dict_list]\n","test_dict_len = [len(item['text']) for item in train_dict_list]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lRiKvq2kA-LI","outputId":"1eaf812f-cf11-4376-f45a-f9b72872b045"},"outputs":[{"name":"stdout","output_type":"stream","text":["29\n","29\n","29\n"]}],"source":["print(max(train_dict_len))\n","print(max(valid_dict_len))\n","print(max(test_dict_len))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ix_-fqVDA-LI","outputId":"ba7c1fbc-1ce6-44ab-99d8-ef3df4c09628"},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n","1\n","1\n"]}],"source":["print(min(train_dict_len))\n","print(min(valid_dict_len))\n","print(min(test_dict_len))"]}],"metadata":{"interpreter":{"hash":"68aafbb34820caa8c170c2eed0dd2fd0208edc5fb9fc86a0f609d859bc90385a"},"kernelspec":{"display_name":"Python 3.10.5 64-bit (windows store)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"orig_nbformat":4,"colab":{"name":"save_text_jsonl.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}